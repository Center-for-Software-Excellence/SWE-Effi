[{"id":"guide/update-site-data","section":"Guide / Update Site Data","title":"Update Site Data","description":"How to update the benchmark & leaderboard data on the site","headings":["Leaderboard","Chart pipeline","copies & renames files into website/public/data/benchmark/raw/","writes to website/public/data/chart/"],"slug":"guide/update-site-data"},{"id":"benchmark/agent-scaffold-blog","section":"Benchmark / Agent Scaffold Blog","title":"Beyond Resolve Rate: A New Leaderboard for Real-World SWE Agent Performance","description":"Introducing SWE-Lens, a new leaderboard evaluating AI Software Engineering agents under resource constraints.","headings":["Introduction","Why a New Leaderboard? The Need for Efficiency-Aware Evaluation","Our Contribution: A New Lens, Not a New Landscape","Experimental Settings: How We Built the Leaderboard: Our Methodology","Key Findings & Initial Insights","Finding 1: The Efficiency King and the \"Early Strike\" Strategy","Finding 2: The Paradox of \"Reasoning\" Models: Fewer, Smarter Calls Win the Day","Finding 3: The \"Token Snowball\": A Hidden Cost That Cripples Inefficient Agents","Finding 4: The Cost of Failure: Not All Agents Fail Equally","Finding 6: The Ultimate Bottleneck for a Learning Agent: Why CPU Time is the True Predictor of Evolvability","Appendix","Appendix 1. Logging Parsing Steps"],"slug":"benchmark/agent-scaffold-blog"},{"id":"leaderboard","section":"Leaderboard","title":"Holistic Evaluation of LLM-Based SWE Scaffolds","headings":["Leaderboard","Analytics","Partners","Citations"],"description":"Leaderboards","slug":"/","tags":["benchmark","leaderboard","LLM"]}]