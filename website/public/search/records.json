[{"id":"/guide/update-site-data","section":"Guide / Update Site Data","title":"Update Site Data","description":"Procedure for refreshing benchmark & leaderboard data displayed on the site","headings":["Overview","1. Collect raw statistics","2. Copy & normalize the raw data","3. Generate chart & leaderboard data"],"slug":"/guide/update-site-data"},{"id":"/benchmark/agent-scaffold-blog","section":"Benchmark / Agent Scaffold Blog","title":"Beyond Resolve Rate: A New Leaderboard for Real-World SWE Agent Performance","description":"Introducing SWE-Lens, a new leaderboard evaluating AI Software Engineering agents under resource constraints.","headings":["Introduction","Why a New Leaderboard? The Need for Efficiency-Aware Evaluation","Our Contribution: A New Lens, Not a New Landscape","Experimental Settings: How We Built the Leaderboard: Our Methodology","Key Findings & Initial Insights","Finding 1: The Efficiency King and the \"Early Strike\" Strategy","Finding 2: The Paradox of \"Reasoning\" Models: Fewer, Smarter Calls Win the Day","Finding 3: The \"Token Snowball\": A Hidden Cost That Cripples Inefficient Agents","Finding 4: The Cost of Failure: Not All Agents Fail Equally","Finding 6: The Ultimate Bottleneck for a Learning Agent: Why CPU Time is the True Predictor of Evolvability","Appendix","Appendix 1. Logging Parsing Steps"],"tags":["agent scaffold blog"],"slug":"/benchmark/agent-scaffold-blog"},{"id":"leaderboard","section":"Leaderboard","title":"Holistic Evaluation of LLM-Based SWE Scaffolds","headings":["Leaderboard","Analytics","Partners","Citations"],"description":"Leaderboards","slug":"/","tags":["benchmark","leaderboard","LLM"]}]