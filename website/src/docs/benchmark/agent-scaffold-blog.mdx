---
title: 'Re-Evaluating SWE Agent Scaffold with Constrained Resources.'
description: Introducing SWE-Effi, a new leaderboard evaluating AI Software Engineering agents under resource constraints.
date: 2025-03-26
tags: ['agent scaffold blog']
---

import { Divider } from '@/components/common/ui/divider';
import { CostBarChart } from '@/components/docs/leaderboard/chart/cost-bar-chart';
import { MetricsRadarChart } from '@/components/docs/leaderboard/chart/metrics-radar-chart';
import { TimePercentageBarChart } from '@/components/docs/leaderboard/chart/time-percentage-bar-chart';
import TablesCard from '@/components/docs/leaderboard/tables-card';
import { getLeaderboardUIConfig } from '@/config/ui/leaderboard';

> **Problem**: AI coding assistant leaderboards (e.g., SWE-bench) focus solely on "resolve rate," ignoring the crucial factor of efficiency in a resource-constrained world.
> Our Solution: We introduce SWE-EffiMetric, a new leaderboard that re-evaluates agents based on a holistic efficiency score. We define efficiency as the balance between resolve rate (the outcome) and the resources consumed (token cost and execution time). We re-ranked popular agent scaffold on a subset of SWE-bench using this new, multi-dimensional metric.

> **Key Finding**: We found that most agents achieve their successful fixes with high initial efficiency (under 0.5M tokens). However, they "fail expensively," wasting enormous resources on problems they will never solve, dragging down their overall efficiency score.

## Introduction

Leaderboards for benchmarks like SWE-bench provide a foundation for measuring the progress of AI coding assistants in repository-level SWE tasks, such as resolving issues submitted by developers. Yet, the spotlight on these leaderboards shines almost exclusively on a single metric: the resolve rate.

Current evaluation paradigms often operate on the implicit assumption of unlimited resources, but companies and developers live in a resource-constrained reality. Was the solution found in minutes, or did it burn through hours of expensive GPU time? Can a team even afford to run this agent on a daily basis? By ignoring such questions, current benchmarks create a disconnect from the realities of software development, where a solution should be more than correct—it must also be cost-effective.

This perspective shift is critical for two key frontiers in AI-driven software development.

First, a trend emerges where agents consume massive LLM tokens (e.g., by test-time compute) to resolve one particular SWE task and earn marginal gains on leaderboards. This raises a critical question about the law of diminishing returns: Is a 1% improvement in resolve rate worth a 5x increase in cost? We argue that this path may not be the most promising direction for building truly scalable and accessible tools.

Second, agent scaffolds are increasingly being used as foundations by research teams to train smarter, self-improving models that specialize for SWE tasks through Reinforcement Learning (RL). In long-horizon RLs (like DeepResearch Agent and SkyRL training), they usually have efficiency requirements so that the RL process won't run forever. In the SWE task scenario, each RL training step, or "rollout," involves setting up the project environment, LLM reasoning, code execution, and test validation. If an agent scaffold is slow or expensive, it hinders the entire RL process. High latency per rollout becomes a significant barrier to effective RL training, limiting progress across the community.

For instance, DeepSWE developed a custom scaffold to ensure low-latency performance for RL. The Sky-RL project uses OpenHands but requires extensive optimizations to make training feasible. Similarly, SWE-RL employs the lightweight Agentless-Mini yet avoids direct execution feedback in RL, likely due to high execution costs. These challenges arise because RL relies on massive-scale trial and error, directly affected by the scaffold's performance.

To bridge this gap, we introduce SWE-EffiMetric leaderboard. While not a new benchmark, we re-evaluate and analyze agent scaffolds on top of the well-established SWE-bench. SWE-EffiMetric offers a holistic perspective on agent performance and highlights performance under resource constraints, emphasizing cost-effectiveness. Moreover, SWE-EffiMetric serves as a valuable indicator for identifying promising foundations in Reinforcement Learning for SWE tasks.

## Our Contribution

To be clear, SWE-EffiMetric is designed to complement existing benchmarks like SWE-bench. Our primary contribution is a new evaluation perspective, metrics, and a public leaderboard. We conducted an exploratory study, which analyzed five well-known agent scaffoldings (AutoCodeRover, SWE-agent, OpenHands, Agentless, Agentless-mini) paired with three different LLMs (GPT-4o-mini, Llama-3.3-70b, Qwen3-32b) on a representative subset of SWE-bench issues. While we tracked the final resolve rate, our main focus was on evaluating the agent scaffold’s time efficiency in resolving issues and analyzing the characteristics behind it.

The findings we present are not intended to be a final, definitive verdict on these five scaffolds. Rather, they serve as an example of the deeper understanding we can unlock by looking at the same problems from a different perspective.

## Experimental Settings

To ensure our leaderboard provides meaningful and fair comparisons, we meticulously selected the baseline agent scaffolds and LLMs and controlled the experiment environment as described below.

Scaffold Selection and Configuration: We selected five popular, actively maintained open-source agents from the top of the SWE-bench leaderboard: AutoCodeRover, Open-Hands, SWE-agent, Agentless, and Agentless-mini. \[insert introduction for each scaffold\] We configured the agents based on their official guidelines and adhered to their default iteration or generation limits to provide a baseline for "out-of-the-box" performance, except for the termination criteria for SWE-agent. For SWE-agent, we adjusted its API budget to a strict $1 per issue. This forces the agent to operate under the same kind of financial pressure a real team would face.

To ensure our time duration measurements were accurate and unbiased, we explicitly disabled any parallel processing capabilities for the five agent scaffolds. This allowed us to measure the true, sequential processing time of each agent's core logic, making our time-based comparisons direct and fair.

LLM Selection: We selected models that represent a practical balance of performance and cost, making them suitable for production environments where budgets are a key consideration. To analyze performance across different resource conditions, our selection covers a range of types and sizes:

- GPT-4o-mini: A proprietary model from OpenAI, designed for high efficiency and speed. It serves as a baseline for performance in cost-sensitive scenarios.
- Llama-3.3-70b: A large, open-source model from Meta. Its 70B parameter size provides strong, general-purpose capabilities and represents a high-performance option within the open-source ecosystem.
- Qwen3-32b: A mid-sized, 32B parameter open-source model from Alibaba, noted for its reasoning abilities. We selected it specifically for its strong reasoning abilities relative to its size. This allows us to evaluate the performance of a more lightweight but capable alternative, which is an attractive profile for resource-constrained environments.

To use these models, we accessed GPT-4o-mini via the OpenAI API, Llama-3.3-70b via Together.AI's inference service, and Qwen3-32b via Alibaba Cloud's API. We modified the agent scaffoldings where necessary to handle specific API behaviors, such as the streaming-only output of Qwen3-32b, to ensure consistent operation.

Benchmark: We evaluated all agent-model combinations on a focused subset of 50 issues randomly drawn from the well-respected SWE-bench-Verified dataset. To ensure this subset was a fair representation of the whole, we used stratified sampling, preserving the original distribution of issues across different software projects. This approach ensures our findings are representative while keeping the experiment manageable.

To capture the fine-grained data for our analysis, we augmented each agent's logging and added our own profiling code (more details in the Appendix) to collect the precise time cost regarding local CPU computation and the backend LLM inference for each agent scaffold.

A major obstacle in evaluating agents is that raw inference time is not a fair metric. It depends heavily on the service provider's hardware (e.g., OpenAI vs. Together.AI), the specific GPU used, and transient server load. A direct comparison of wall-clock times between agents using different backends LLM would be an apples-to-oranges comparison, reflecting the provider's infrastructure as much as the agent's efficiency.

To mitigate this, we introduce a standardized time metric called Normalized Inference Time. This metric estimates how long an LLM call would have taken if it were run on a single, consistent baseline, thereby creating a fair, hardware-agnostic basis for comparison.

Establish a Baseline: We chose GPT-4o-mini (served via OpenAI's API) as our performance baseline, representing a common and efficient proprietary model.

Model the Baseline's Performance: We conducted a linear regression analysis on hundreds of API calls to GPT-4o-mini. This allowed us to build a predictive model that correlates the number of input tokens (prefill phase) and output tokens (decoding phase) to the observed wall-clock inference time. The resulting model is:
Normalized Inference Time = β₀ + β₁(Input Tokens) + β₂(Output Tokens)

Where β₀, β₁, and β₂ are the regression coefficients derived from our baseline measurements.
Apply the Model Universally: For every LLM call made by any agent (e.g., using Llama-3.3 or Qwen3), we plug its input and output token counts into this formula.

$Adjusted Average Total Time (aatt) = CPU measure time + Normalized Inference Time$

Dimensions:

- $Resolve rate \rightarrow effectiveness$
- $AUC (num_resolve_case vs total_token) \rightarrow token cost efficiency$
- $AUC (num_resolve_case vs adjusted_inference_time) \rightarrow inference cost efficiency$
- Ranking parameter: $\beta_0, \beta_1, \beta_2$
- $AUC (num_resolve_case vs CPU_measured_time) \rightarrow CPU cost efficiency$
- $AUC (num_resolve_case vs total_token * normalized_token_cost) \rightarrow price cost efficiency$
  - Use open router standard price, with a note about our price source for experiment

$Rank factor = sum(6 dimensions)$

In summary, we re-evaluate the baseline agent scaffolds by using the below metrics.

- Adjusted Time: k.vasileuski@gmail.com Please add the calculation.
- Distinguishing Agent Logic from Model Latency: We measured the total wall-clock time, but more importantly, we distinguished between time spent waiting for the LLM to "think" (inference latency) and time the agent spent "doing" work locally (CPU processing time), such as running tests or using tools. This allows us to understand where an agent spends its time.

- A Standardized View of Cost and Workload: We recognized that raw inference time can be misleading, as it depends heavily on the service provider's hardware and current load. To create a fair, hardware-agnostic comparison of the "work" required from the LLM, we focused on two key metrics:
  - Standardized Computational Cost (Total Tokens): We use the total number of tokens (input + output) as the primary, standardized metric for an agent's computational cost. This value represents the fundamental workload an agent places on the language model, and it is the most direct driver of financial cost, regardless of which provider is used.
  - Request Overhead (API Call Count): In addition to the sheer volume of tokens, we also tracked the total number of API calls. An agent that makes many small requests may incur different kinds of latency and overhead compared to one that makes a few large requests.

By combining this setup with our detailed measurement, we have created a leaderboard that reflects not just success, but the true cost of that success. The following sections present the data and our initial findings.

<CostBarChart
  className="lg:p-0"
  {...getLeaderboardUIConfig().analytics.costBarChart}
/>
<br />
<TimePercentageBarChart
  {...getLeaderboardUIConfig().analytics.timePercentageBarChart}
  className="lg:p-0"
/>
<br />
<MetricsRadarChart
  {...getLeaderboardUIConfig().analytics.metricsRadarChart}
  className="lg:p-0"
/>

## Key Findings & Initial Insights

#### Finding 1

Table 1 presents a quantitative result of the five selected agent scaffolds, each paired with three distinct Large Language Models (LLMs), evaluated on a curated subset of the SWE-bench-Verified benchmark. The metrics reported are aggregated across all issue instances, including both resolved and unresolved issues. The columns are defined as follows:

- Avg CPU Time (seconds): This metric quantifies the mean computational time consumed by the agent's scaffolding for local operations, exclusive of LLM inference latency. It serves as a proxy for the framework's intrinsic overhead and architectural complexity.
- Avg Inf. Cost (Average Inference Cost): This section delineates the computational workload delegated to the LLM.
  - Token (k): Specifies the mean number of tokens (in thousands) processed per trial, presented as a tuple (Input, Output). This value is a direct correlate of the financial expenditure associated with API utilization.

  - \# Calls: Denotes the mean frequency of API requests to the LLM per trial. This metric is a key variable in our efficiency analysis, as a high request frequency is hypothesized to induce super-linear cost scaling effects.

- % Issue Resolved (by million Token): This set of columns evaluates the cumulative resolution rate as a function of token consumption. Each sub-column (e.g., 0.1m, 0.25m) represents a token budget ceiling, reporting the percentage of total issues successfully resolved within that budget. This cumulative distribution function allows for the analysis of an agent's marginal utility of token investment.

Our initial experiments uncovered several crucial trends and trade-offs that are vital for anyone looking to build, deploy, or research AI software engineering agents. Here are our key findings:

<TablesCard
  tablesUi={getLeaderboardUIConfig().tables}
  className="md:px-0"
  caption={true}
/>

#### Finding 2

Counter-intuitively, the reasoning model that consumes significantly more tokens per call can resolve issue instances with fewer total tokens. Table 1 shows that using a smaller, specialized "reasoning" model like Qwen3-32b was significantly token-efficient than using a larger, general-purpose model like Llama-3.3-70b. While a reasoning model may be more intensive per call, it enables the agent to solve problems with fewer API calls (e.g., 15 calls for Qwen vs. 38 for Llama with AutoCodeRover). This reveals a critical insight: the primary driver of an agent's total cost is the number of interactions, not the intensity of each one. A smarter model that can formulate a more concise strategy saves vast amounts of tokens and time by avoiding unnecessary back-and-forth communication.

#### Finding 3: Agent Scaffold lacks a smart termination strategy

Our analysis reveals that current agentic-based agent scaffolds lack smart termination strategies when they get stuck. Scaffold like SWE-agent operates on a simple termination logic: they stop only when they either solve the problem, hit an economic budget, or LLM API requests limit.

Our data exposed a fatal flaw in agents that rely on too many API calls: the "token snowball effect." Scaffolds like SWE-Agent, when making hundreds of calls, can cause their input token costs to grow exponentially as conversation history accumulates. In one extreme case, an agent consumed over 8 million input tokens to get just 24,000 output tokens—a staggering 300:1 overhead. This demonstrates that intelligent context and memory management is not a feature, but a core architectural requirement for any economically viable agent. Without it, an agent is destined to drown in its own verbosity.

#### Finding 4: Failing is Far More Expensive Than Succeeding ~Not All Agents Fail Equally~

Our analysis of resolved (R) versus unresolved (U) issues reveals a critical trend across the board: current agents do not know when to quit. For nearly every combination, the resources spent on a problem the agent failed to solve were significantly higher than on one it successfully resolved.

This "Fail Expensively" behavior is a defining characteristic of the current generation of agents. Let's look at the evidence from Table 2:

- For SWE-Agent with GPT-4o-mini, a failed attempt (U) cost an astronomical 8.8 million tokens and 864 seconds. In contrast, a successful fix (R) cost a "mere" 1.8 million tokens and 206 seconds. The agent spent over 4 times the resources when it was on the wrong path.
- This isn't unique to one agent. OpenHands with Llama-3.3-70B spent 308 seconds on failures versus 112 seconds on successes. Even the more efficient AutoCodeRover with Llama-3.3-70B spent nearly three times as long on a failed task (150 seconds) as on a successful one (55 seconds).

# TODO: Remove the mentioned Seconds here

This pattern suggests a fundamental lack of "futility detection." When an agent finds a viable path, it can sometimes solve the problem relatively directly. But when it's stuck, it enters costly, unproductive loops, repeatedly trying failing strategies until an external limit (like time or budget) is hit. This is a massive hidden cost and risk in real-world deployment.
