---
title: Submit Your Benchmark Results
description: Procedure for submitting your benchmark results
date: 2025-07-22
---

## Before You Submit

### Prerequisites Checklist

- You've completed a full benchmark evaluation
- Your scaffold/model has a descriptive name

### File Format Requirements

You'll need to prepare two JSON files:

#### `combined_stats.json`

```json
{
  "astropy__astropy-14369": {
    "input_tokens": 40988,
    "output_tokens": 63645,
    "cpu_time": 740.4647353969574,
    "gpu_time": 321.34873104014207,
    "resolved": false,
    "duration": 1061.8134664370996,
    "llm_calls": 85,
  },
  "astropy__astropy-8707": {
    "input_tokens": 22223,
    "output_tokens": 36313,
    "cpu_time": 634.6776038490295,
    "gpu_time": 183.92350658239107,
    "resolved": false,
    "duration": 818.6011104314206,
    "llm_calls": 85,
  },
...
}
```

#### `summary_stats.json`

```json
{
  "total_projects": 50,
  "total_tokens": 3367323.0,
  "avg_total_tokens": 67346.46,
  "resolved": 24,
  "avg_resolved": 0.48,
  "input_tokens": 1298273.0,
  "avg_input_tokens": 25965.46,
  "output_tokens": 2069050.0,
  "avg_output_tokens": 41381.0,
  "llm_calls": 4153.0,
  "avg_llm_calls": 83.06,
  "cpu_time": 36395.753749303825,
  "avg_cpu_time": 727.9150749860765,
  "gpu_time": 10470.830791685958,
  "avg_gpu_time": 209.41661583371916,
  "duration": 46866.584540989774,
  "avg_duration": 937.3316908197954,
  "resolved_total_input_tokens": 590340.0,
  "resolved_avg_input_tokens": 24597.5,
  "resolved_total_output_tokens": 714606.0,
  "resolved_avg_output_tokens": 29775.25,
  "resolved_total_llm_calls": 2005.0,
  "resolved_avg_llm_calls": 83.54166666666667,
  "resolved_total_cpu_time": 16264.238418299105,
  "resolved_avg_cpu_time": 677.6766007624627,
  "resolved_total_gpu_time": 3632.2676034990836,
  "resolved_avg_gpu_time": 151.34448347912848,
  "resolved_total_duration": 19896.506021798188,
  "resolved_avg_duration": 829.0210842415912,
  "unresolved_total_input_tokens": 707933.0,
  "unresolved_avg_input_tokens": 27228.19230769231,
  "unresolved_total_output_tokens": 1354444.0,
  "unresolved_avg_output_tokens": 52094.0,
  "unresolved_total_llm_calls": 2148.0,
  "unresolved_avg_llm_calls": 82.61538461538461,
  "unresolved_total_cpu_time": 20131.51533100471,
  "unresolved_avg_cpu_time": 774.2890511924888,
  "unresolved_total_gpu_time": 6838.563188186874,
  "unresolved_avg_gpu_time": 263.0216610841105,
  "unresolved_total_duration": 26970.07851919159,
  "unresolved_avg_duration": 1037.3107122765996,
  "precision": 0.4897959183673469,
  "token_efficiency_auc": 0.46714219,
  "cost_efficiency_auc": 0.471231735,
  "cpu_efficiency_auc": 0.3027765182380528,
  "gpu_efficiency_auc": 0.2993275853647659
}
```

## Validation Tools

Before submitting, you can validate your files locally:

### Step 1: Get the Repository

```bash
git clone https://github.com/Center-for-Software-Excellence/SWE-Effi.git
cd swe-effi
```

### Step 2: Add Your Files

```bash
# Create directory for your results
mkdir -p benchmark/results/agent-scaffold-stats/my-scaffold-name/my-model-name/

# Copy your result files
cp /path/to/your/combined_stats.json benchmark/results/agent-scaffold-stats/my-scaffold-name/my-model-name/
cp /path/to/your/summary_stats.json benchmark/results/agent-scaffold-stats/my-scaffold-name/my-model-name/
```

### Step 3: Validate Format

```bash
python3 scripts/transform-benchmark.py --validate-only --verbose
```

If validation passes, you're ready to submit! If not, check the error messages and fix any formatting issues.

## Submission Process

### Method 1: GitHub Pull Request (Recommended)

1. **Fork our repository** on GitHub

2. **Clone your fork:**

   ```bash
   git clone https://github.com/YOUR-USERNAME/swe-effi.git
   cd swe-effi
   ```

3. **Create a feature branch:**

   ```bash
   git checkout -b add-results-my-scaffold-my-model
   ```

4. **Add your result files** to the correct location:

   ```
   benchmark/results/agent-scaffold-stats/
   └── my-scaffold-name/
       └── my-model-name/
           ├── combined_stats.json
           └── summary_stats.json
   ```

5. **Commit your changes:**

   ```bash
   git add benchmark/results/agent-scaffold-stats/my-scaffold-name/
   git commit -m "Add benchmark results for my-scaffold with my-model"
   ```

6. **Push to your fork:**

   ```bash
   git push origin add-results-my-scaffold-my-model
   ```

7. **Create a Pull Request** on GitHub

### Method 2: Issue Submission

If you're not comfortable with Git, you can:

1. Create a new issue in our repository
2. Use the "Benchmark Submission" template
3. Attach your JSON files or paste their contents
4. We'll handle the integration for you

## Pull Request Template

When creating your PR, please include this information:

```markdown
## Benchmark Results Submission

**Scaffold Name**: [Your scaffold name]
**Model Used**: [e.g., GPT-4o-mini-2024-07-18, qwen3-32B]

### Checklist

- Files pass validation (`python3 scripts/transform-benchmark.py --validate-only`)
- Results are from a complete benchmark run
- JSON files follow the required format
- Scaffold and model names are descriptive and consistent
```

## Recognition

Accepted submissions will:

- Appear on the main leaderboard
- Be included in analytics section

Ready to contribute? **[Start your submission →](https://github.com/Center-for-Software-Excellence/SWE-Effi/fork)**
